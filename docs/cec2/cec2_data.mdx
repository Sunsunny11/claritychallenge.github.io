---
id: cec2_data
title: CEC2 Data
sidebar_label: CEC2 Data
sidebar_position: 5
---

To obtain the data and baseline code, please see the download page.

## A. Training, development, evaluation data

The dataset is split into these three subsets: training (`train`), development (`dev`) and evaluation (`eval`).

- You should only train on the training set, though you are allowed to augment it.
- The system submitted should be chosen on the evidence provided by the development set.
- The final listening and ranking will be performed with the (held-out) evaluation set.
- For more information on supplementing the training data, please see the rules. The evaluation dataset will be made available one month before the challenge submission deadline.

## B. The scene dataset

The complete dataset is composed of 10,000 scenes split into the following sets:

- Training (6000 scenes, 24 speakers);
- Development (2500 scenes, 10 speakers);
- Evaluation (1500 scenes, 6 speakers).

Each scene corresponds to a unique target utterance and a unique segment of noise from one, two or three interferers. The training, development and evaluation sets are disjoint for target speaker. The three sets are balanced for target speaker gender.

High-Order Ambisonic Impulse Responses (HOA-IRs) and Head-Related Impulse Response (HRIRs) are used to model how the sound is altered as it propagates through the room and interacts with the head. The audio signals for the scenes are generated by convolving source signals with the BRIRs and summing. See the page on modelling the scenario for more details. Randomised room dimensions, target and interferer locations are used.


Time-domain acoustic signals are generated for:

- A hearing aid with 3 microphone inputs (front, mid, rear). The hearing aid has a Behind-The-Ear (BTE) form factor; see Figure 1. The distance between microphones is approx. 7.6 mm. The properties of the tube and ear mould are not considered.
- Close to the eardrum.
- The anechoic target reference (front microphone).

<img src="../../img/tutorial/BTE-e1606144768702.png" width="200" />

*Figure 1. Front (Fr), Middle (Mid) and Rear microphones on a BTE hearing aid form.*

Head Related Impulse Responses (HRIRs) are used to model how sound is altered as it propagates in a free-field and interacts with the head (i.e., no room is included). These are taken from the OlHeadHRTF database with permission. These include HRIRs for human heads and for three types of head-and-torso simulator/mannekin. The eardrum HRIRs (labelled ED) are for a position close to the eardrum of the open ear.

`rpf` files and `ac` files are specification files for the geometric room acoustic model that include a complete description of the room, both in terms of geometry and room materials.

### B.1 Training data

For each scene in the training data the following signals and metadata are available:

- The target and interferer BRIRs (4 pairs: front, mid, rear and eardrum for left and right ears).
- HRIRs including those corresponding to the target azimuth.
- The mono target and interferer signals (pre-convolution).
- For each hearing aid microphone (channels 1-3 where channel 1 is front, channel 2 is mid and channel 3 is rear) and a position close to the eardrum (channel 0):
   - The target convolved with the appropriate BRIR;
   - The interferer convolved with the appropriate BRIR;
   - The sum of the target and interferer convolved.
- The target convolved with the anechoic BRIR (channel 1) for each ear (‘target_anechoic’).
- Metadata describing the scene: a JSON file containing, e.g., the filenames of the sources, the location of the sources, the viewvector of the target source, the location and viewvector of the receiver, the room dimensions (see specification below), and the room number, which corresponds to the RAVEN BRIR, rpf and ac files.

Software for generating more training data is also available.

### B.2 Development data

The same data as for the training will be made available to allow you to fully examine the performance of your system. Note, that the data available for the evaluation will be much more limited (see B.3).

For each scene, during development, your hearing aid enhancement model must only use the following input signals/data:

- The sum of the target and interferers – mixed at the SNR specified in the scene metadata – at one or more hearing aid microphones (CH1, CH2 and/or CH3).
- The IDs of the listeners assigned to the scene in the metadata provided.
- The provided listener characterisation data (audiograms, digit triple test performance, etc).


### B.3 Evaluation scene data 

For each scene in the evaluation data only the following will be available:

- The sum of the target and interferers for each hearing aid microphone.
- The ID of the evaluation panel members/listeners who will be listening to the processed scene.
- The listener characterisation data for these listeners.

## C Listener data

### C.1 Training and development data

NEEDS UPDATING: A sample of pure tone air-conduction audiograms that characterise the hearing impairment of potential listeners, split into training and development sets.

### C.2 Evaluation data

A panel of up to 50 hearing-aided listeners will be recruited for evaluation. We plan that they will be experienced bilateral hearing-aid users (they use two hearing aids but the hearing loss may be asymmetrical) with an averaged hearing loss as measured by pure tone air-conduction of between 25 and about 60 dB in the better ear, with fluent speaking of (and listening to) British English.

You will be provided with characterisation data for the listening panel, i.e. the left and right pure tone air-conduction audiograms and digit-triple test performance, so the signals you generate for evaluation can be individualised to the specific listeners who will be hearing them.

## D Data file formats and naming conventions

### D.1 Abbreviations in Filenames

The follow abbreviations are used consistently throughout the CEC2 filenames and references in the metadata.

- `R` – “room”:  e.g., “R02678”   # Room ID linking to RAVEN rpf file
- `S` –  “scene”:  e.g., S00121  # Scene ID for a particular setup in a room I.e., room + choice of target and interferer signals
- `BNC` – BNC sentence identifier  e.g. BNC_A06_01702
- `CH` –
  - `CH0` – eardrum signal
  - `CH1` – front signal, hearing aid channel
  - `CH2` – middle signal, hearing aid channel
  - `CH3` – rear signal, hearing aid channel
- `I`/`i1` – Interferer, i.e., noise or sentence ID for the interferer/masker
- `T` – talker who produced the target speech sentences
- `L` – listener
- `E` – entrant (identifying a team participating in the challenge)
- `t` – target (used in BRIRs and RAVEN project ‘rpf’ files)

### D.2 General 

- Audio and HOA-IRs will be 44.1 kHz 32-bit wav files in either mono or stereo as appropriate.
- Where stereo signals are provided the two channels represent the left and right signals of the ear or hearing aid microphones.
- Metadata will be stored in JSON or csv format as appropriate with the exception of
  - Room descriptions are stored as RAVEN project ‘rpf’ configuration files and ‘ac’ files. (However, key details are reflected in the scene.json files).
- Signals are saved within the Python code as 32-bit floating point by default.
- Output signals for the listening tests will be required to be in 16-bit format.

### D.3 Prompt and transcription data

The following text is available for the target speech:

- Prompts are the text that was supposed to be spoken as presented to the readers.
- ‘Dot’ transcriptions contain the text as it was spoken in a form more suitable for scoring tools.
- These are stored in the master json metadata file.

### D.4 Source audio files
 
- Wav files containing the original source materials.
- Original target sentence recordings:

```
  <Talker ID>_<BNC sentence identifier>.wav
```

### D.5 Preprocessed scene signals

Audio files storing the signals picked up by the hearing aid microphone ready for processing. Separate signals are generated for each hearing aid microphone pair or ‘channel’.

```
<Scene ID>_target_<Channel ID>.wav
<Scene ID>_interferer_<Channel ID>.wav
<Scene ID>_mixed_<Channel ID>.wav
<Scene ID>_target_anechoic.wav
```

`Scene ID` –  S00001  to S10000

- `S` followed by 5 digit integer with 0 pre-padding

`Channel ID`   

- `CH0` – Eardrum signal
- `CH1` – Hearing aid front microphone
- `CH2` – Hearing aid middle microphone
- `CH3` – Hearing aid rear microphone

### D.6 Enhanced signals 

The signals that are output by the enhancement (hearing aid) model.

- `<Scene ID>_<Listener ID>_HA-output.wav` #HA output signal (i.e., as submitted by the challenge entrants)

`Listener ID`  – ID of the listener panel member,  e.g., L001 to L100 for initial ‘pseudo-listeners’, etc.  We are no longer providing the script for post-processing signals in preparation for the listener panel.

### D.7 Scene metadata

JSON file containing a description of the scene. It is list of dictionaries with each entry representing a unique scene.

```json
[
  {
    "room": {
      "name": "R00001", # ID of room linking to RAVEN rpf and ac files
      "dimensions": "6.9933x3x3" # Room dimensions in metres
    },
    "target": { # target positions (x,y,z) and view vectors (look directions, x,y,z)
      "position": [
        -0.3,
        2.4,
        1.2
      ],
      "view_vector": [
        0.071,
        0.997,
        0.0
      ],
      "name": "T005_JYD_04274", # target speaker code and BNCid
      "time_start": 107210, # start time of target in samples
      "time_end": 217019 # end time of target in samples
    },
    "listener": {
      "position": [
        -0.1,
        5.2,
        1.2
      ],
      "rotation": [ # Defines the head motion - list of time, direction pairs
        {
          "sample": 88200,
          "view_vector": [
            -1,
            0,
            0.0
          ]
        },
        {
          "sample": 176400,
          "view_vector": [
            0,
            0,
            1
          ]
        }
      ],
      "hrir_filename": "VP_E9-ED" # HRIR filename
    },
    "interferers": [
      {
        "position": [
          0.4,
          4.0,
          1.2
        ],
        "time_start": 0, # time of interferer onset in samples
        "time_end": 261119, # time of interferer offset in samples
        "name": "track_1353255", # interferer name
        "type": "music", # interferer type: speech, noise or music
        "offset": 4076256 # index into interferer file at which to extract sample
      },
      { # etc, up to three interferers
      }
    ],
    "scene": "S00001",  # the unique scene ID
    "dataset": "train", # the dataset to which the scene belongs: train, dev or eval
    "duration": 261119,  # total duration of scene in samples
    "SNR": 6.89  # targe SNR for the scene
  },
  { etc }
]
```

- There are JSON files containing the scene specifications per dataset, e.g., scenes.train.json.- Note, that the scene ID and room ID might have a one-to-one mapping in the challenge, but are not necessarily the same. Multiple scenes can be made by changing the target and masker choices for a given room. E.g., participants wanting to expand the training data could remix multiple scenes from the same room.
- is not stored in the scene metadata; this information is stored separately in a `scenes_listeners.json` file which maps scenes to listeners, ie. telling you which listener (or listeners) will be listening to which scenes in the evaluation. (See Section D9)
- Noise interferers are labelled `CIN_<noise type>_XXX`; music intereferers ... TODO;  while speech interferers are labelled `<three letter code including dialect and talker gender>_XXXXX` .

### D.8 Listener metadata

Audiogram data is stored in a single JSON file with the following format.

```json
{“L0001”: {
    “name”: “L0001”,
"audiogram_cfs": [250, 500, 1000, 2000, 3000, 4000, 6000, 8000],
    “audiogram_levels_l”: [10, 10, 20, 30, 40, 55, 55, 60],
    “audiogram_levels_r”: [ … ],
          },
“L0002”: {
    },
...
}
```

Additional metadata (e.g. digit triple test results) are stored in a csv file. DETAILS

### D.9 Scene-Listener map

JSON file named `scenes_listeners.json` dictates which scenes are to be processed by which listeners.

```json
{“S00001”: [“L0001”, “L0002”, “L0003”],
“S00002”: [“L0003”. “L0005”, “L0007”],
etc
}
```
