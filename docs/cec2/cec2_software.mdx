---
id: cec2_software
title: Software
sidebar_label: Software
sidebar_position: 6
---
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

The code is provided as a GitHub repository containing individual Python tools and a complete baseline system. Tools will allow the processing of individual scenes or the bulk processing of the complete Clarity dataset.

The key elements of the baseline system are the

- Scene generator
- Hearing aid processor baseline
- HASPI-based Speech intelligibility model

Other tools include a hearing loss model, differentiable source separation and hearing aid amplification modules, alternative intelligibility models etc.

## A. Scene generator

Fully open-source python code for generating hearing aid inputs for each scene

- **Inputs**: target and interferer signals, HOA-IRs, RAVEN project (rpf) files, scene description JSON files
- **Outputs**: Mixed target+interferer signals for each hearing aid channel, direct path (simulating a measurement close to the eardrum). Reverberated pre-mixed signals can also be optionally generated.

## B. Baseline hearing aid processor

TODO

The baseline hearing aid processor is based on openMHA. The python code configures openMHA with a Camfit compressive fitting for a specific listener’s audiogram. This includes a python implementation of the Camfit compressive prescription and python code for driving openMHA.

This configuration of openMHA includes multiband dynamic compression and non-adaptive differential processing. The intention was to produce a basic hearing aid without various aspects of signal processing that are common in high-end hearing aids, but tend to be implemented in proprietary forms so cannot be replicated exactly.

The main inputs and outputs for the processor are as follows:

- **Inputs**: Mixed scene signals for each hearing aid channel, a listener ID drawn from scene-listener pairs identified in ‘scenes_listeners.json’ and an entry in the listener metadata json file ‘listeners.json’ for that ID
- **Outputs**: The stereo hearing aid output signal, `<scene>_<listener>_HA-output.wav`

## C. HASPI speech intelligibility model

Python implementation of the Hearing Aid Speech Perception Index (HASPI) model which is used for objective intelligibility estimation. This will be used in the stage 1 evaluation of entrants (see Rules). Note that HASPI requires signal time-alignment (and alignment within one-third octave bands).

- **Inputs**: TODO HA input signals, audiogram, reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections “turned off”, specified as ‘target_anechoic’), scene metadata
- **Outputs**: predicted intelligibility score
